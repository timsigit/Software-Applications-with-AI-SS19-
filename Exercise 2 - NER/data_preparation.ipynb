{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and read JSON-file\n",
    "import os\n",
    "dataset_path = \"Exercise 2 - Named-Entity-Recognition in Resumes - Data Set.json\"\n",
    "\n",
    "with open(dataset_path,encoding=\"utf8\") as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting JSON encoded data into Python objects using json.loads()\n",
    "import json\n",
    "all_resumes = []\n",
    "\n",
    "for line in lines:\n",
    "    all_resumes.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of coverted resumes:  701\n"
     ]
    }
   ],
   "source": [
    "# data conversion method that creates a list of tuples(resumes) with the original \n",
    "# resume text as String and the entities as Python dictionary\n",
    "def convert_data(data):\n",
    "    text = data['content']\n",
    "    entities = []\n",
    "    if data['annotation'] is not None:\n",
    "        for annotation in data['annotation']:\n",
    "            point = annotation['points'][0]\n",
    "            labels = annotation['label']\n",
    "            if not isinstance(labels, list):\n",
    "                labels = [labels]\n",
    "            for label in labels:\n",
    "                entities.append((point['start'], point['end'] + 1, label))\n",
    "    return (text, {\"entities\": entities})\n",
    "   \n",
    "converted_resumes = [convert_data(res) for res in all_resumes]\n",
    "print(\"Len of coverted resumes: \", len(converted_resumes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "690\n"
     ]
    }
   ],
   "source": [
    "# filter out resumes without annotations\n",
    "new_resumes = []\n",
    "for resume in converted_resumes:\n",
    "    if resume[1][\"entities\"] is not None:\n",
    "        temp = resume[1][\"entities\"]\n",
    "        if temp:\n",
    "            new_resumes.append(resume)\n",
    "\n",
    "converted_resumes = new_resumes\n",
    "print(len(converted_resumes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "521\n"
     ]
    }
   ],
   "source": [
    "# removing all duplicates:\n",
    "# In case of an unequal number of entities within the duplicates, the resume with \n",
    "# the higher number of entities is selected\n",
    "def split_for_name(resume_text):\n",
    "    name = ''\n",
    "    for char in resume_text:\n",
    "        if char == '\\n':\n",
    "            break\n",
    "        else:\n",
    "            name = name + char\n",
    "    return name\n",
    "\n",
    "updated_converted_resumes = converted_resumes[:]\n",
    "\n",
    "for i in range(0, len(converted_resumes)-1):\n",
    "    resume = converted_resumes[i]\n",
    "    resume_name = split_for_name(resume[0])\n",
    "    for j in range(i+1, len(converted_resumes)):\n",
    "        potentialDuplicate = converted_resumes[j]\n",
    "        potential_resume_name = split_for_name(potentialDuplicate[0])\n",
    "        if resume_name == potential_resume_name:\n",
    "            lengthRes = len(resume[1]['entities'])\n",
    "            lengthPot = len(resume[1]['entities'])\n",
    "            if lengthPot > lengthRes:\n",
    "                updated_converted_resumes.remove(resume)\n",
    "                break\n",
    "            else:\n",
    "                updated_converted_resumes.remove(potentialDuplicate)\n",
    "                break\n",
    "\n",
    "print(len(updated_converted_resumes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathered 410 training examples\n"
     ]
    }
   ],
   "source": [
    "resumes = updated_converted_resumes\n",
    "\n",
    "# selection of three labels for our NER-task\n",
    "chosen_entity_labels = ['Degree', 'Designation', 'Companies worked at']\n",
    "\n",
    "def gather_candidates(dataset,entity_labels):\n",
    "    candidates = list()\n",
    "    for resume in dataset:\n",
    "        res_ent_labels = list(zip(*resume[1][\"entities\"]))[2]\n",
    "        if set(entity_labels).issubset(res_ent_labels):\n",
    "            candidates.append(resume)\n",
    "    return candidates\n",
    "\n",
    "training_data = gather_candidates(resumes, chosen_entity_labels)\n",
    "print(\"Gathered {} training examples\".format(len(training_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing all except the chosen entities from the resumes\n",
    "def filter_ents(ents, filter):\n",
    "    filtered = [ent for ent in ents if ent[2] in filter]\n",
    "    return filtered\n",
    " \n",
    "X = []\n",
    "for res in training_data:\n",
    "    ents = (res[1]['entities'])\n",
    "    res[1]['entities'] = filter_ents(ents, chosen_entity_labels)\n",
    "    X.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created blank 'en' model\n",
      "Exception thrown when processing doc:\n",
      "('Nida Khan\\nTech Support Executive - Teleperformance for Microsoft\\n\\nJaipur, Rajasthan - Email me on Indeed: indeed.com/r/Nida-Khan/6c9160696f57efd8\\n\\n• To be an integral part of the organization and enhance my knowledge to utilize it in a productive\\nmanner for the growth of the company and the global.\\n\\nINDUSTRIAL TRAINING\\n\\n• BHEL, (HEEP) HARIDWAR\\nOn CNC System&amp; PLC Programming.\\n\\nWORK EXPERIENCE\\n\\nTech Support Executive\\n\\nTeleperformance for Microsoft -\\n\\nSeptember 2017 to Present\\n\\nprocess.\\n• 21 months of experience in ADFC as Phone Banker.\\n\\nEDUCATION\\n\\nBachelor of Technology in Electronics & communication Engg\\n\\nGNIT institute of Technology -  Lucknow, Uttar Pradesh\\n\\n2008 to 2012\\n\\nClass XII\\n\\nU.P. Board -  Bareilly, Uttar Pradesh\\n\\n2007\\n\\nClass X\\n\\nU.P. Board -  Bareilly, Uttar Pradesh\\n\\n2005\\n\\nSKILLS\\n\\nMicrosoft office, excel, cisco, c language, cbs. (4 years)\\n\\nhttps://www.indeed.com/r/Nida-Khan/6c9160696f57efd8?isid=rex-download&ikw=download-top&co=IN',) ({'entities': [(552, 610, 'Degree'), (420, 449, 'Companies worked at'), (395, 418, 'Designation'), (35, 64, 'Companies worked at'), (10, 33, 'Designation'), (9, 32, 'Designation')]},)\n",
      "Losses {'ner': 47942.22458319829}\n",
      "Unfiltered training data size:  410\n",
      "Filtered training data size:  409\n",
      "Bad data size:  1\n"
     ]
    }
   ],
   "source": [
    "# importing spacy for further data cleaning methods\n",
    "import spacy\n",
    "from spacy_train_resume_ner import train_spacy_ner\n",
    "\n",
    "# removing bad data\n",
    "def remove_bad_data(training_data):\n",
    "    model, baddocs = train_spacy_ner(training_data, debug=True, n_iter=1)\n",
    "\n",
    "    filtered = [data for data in training_data if data[0] not in baddocs]\n",
    "    print(\"Unfiltered training data size: \",len(training_data))\n",
    "    print(\"Filtered training data size: \", len(filtered))\n",
    "    print(\"Bad data size: \", len(baddocs))\n",
    "    return filtered\n",
    "\n",
    "X = remove_bad_data(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "307 102\n"
     ]
    }
   ],
   "source": [
    "# train test split (75% train / 25% test)\n",
    "from random import randrange\n",
    "def train_test_split(X,train_percent):\n",
    "    train_size = train_percent * len(X)\n",
    "    train = list()\n",
    "    test = list(X)\n",
    "    while len(train) < train_size:\n",
    "        index = randrange(len(test))\n",
    "        train.append(test.pop(index))    \n",
    "    return train,test\n",
    "\n",
    "train,test = train_test_split(X, 0.75)\n",
    "assert (len(train) + len(test)) == len(X)  \n",
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created blank 'en' model\n",
      "Losses {'ner': 26532.46284542042}\n",
      "Losses {'ner': 17160.815301885246}\n",
      "Losses {'ner': 39413.8851031012}\n",
      "Losses {'ner': 30389.75278498046}\n",
      "Losses {'ner': 47006.91409990564}\n",
      "Losses {'ner': 26306.692932166625}\n",
      "Losses {'ner': 31602.452643590048}\n",
      "Losses {'ner': 21775.57916584192}\n",
      "Losses {'ner': 16674.040124418563}\n",
      "Losses {'ner': 13480.656986573726}\n",
      "Losses {'ner': 12846.334227504045}\n",
      "Losses {'ner': 8492.267840066816}\n",
      "Losses {'ner': 8122.058526799621}\n",
      "Losses {'ner': 6007.523939203747}\n",
      "Losses {'ner': 6813.648012131569}\n",
      "Losses {'ner': 5038.8486335094085}\n",
      "Losses {'ner': 4316.808869171175}\n",
      "Losses {'ner': 4421.678212810908}\n",
      "Losses {'ner': 4288.890919443016}\n",
      "Losses {'ner': 3699.6698804531425}\n"
     ]
    }
   ],
   "source": [
    "# training of customized spacy model\n",
    "custom_nlp,_= train_spacy_ner(train,n_iter=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.gold import biluo_tags_from_offsets\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# returns a pandas dataframe with tokens, prediction, and true \n",
    "#(Gold Standard) annotations of tokens\n",
    "def make_bilou_df(nlp,resume):\n",
    "    # param nlp - a trained spacy model\n",
    "    # param resume - a resume from our train or test set\n",
    "    doc = nlp(resume[0])\n",
    "    bilou_ents_predicted = biluo_tags_from_offsets(doc, \n",
    "                [(ent.start_char,ent.end_char,ent.label_)for ent in doc.ents])\n",
    "    bilou_ents_true = biluo_tags_from_offsets(doc,\n",
    "                [(ent[0], ent[1], ent[2]) for ent in resume[1][\"entities\"]])\n",
    "\n",
    "    \n",
    "    doc_tokens = [tok.text for tok in doc]\n",
    "    bilou_df = pd.DataFrame()\n",
    "    bilou_df[\"Tokens\"] =doc_tokens\n",
    "    bilou_df[\"Tokens\"] = bilou_df[\"Tokens\"].str.replace(\"\\\\s+\",\"\") \n",
    "    bilou_df[\"Predicted\"] = bilou_ents_predicted\n",
    "    bilou_df[\"True\"] = bilou_ents_true\n",
    "    return bilou_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make bilou dfs\n",
      "Done!\n",
      "(236782, 2)\n",
      "(71685, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def bilou_for_flair(nlp, train, test):  \n",
    "    print(\"Make bilou dfs\")\n",
    "    training_data_as_bilou = [make_bilou_df(nlp,res) for res in train]\n",
    "    test_data_as_bilou = [make_bilou_df(nlp,res) for res in test]\n",
    "    print(\"Done!\")\n",
    "    training_file = pd.DataFrame(columns = [\"text\",\"ner\"])\n",
    "    test_file = pd.DataFrame(columns = [\"text\",\"ner\"])\n",
    "\n",
    "    for idx,df in enumerate(training_data_as_bilou):\n",
    "        df2 = pd.DataFrame()\n",
    "        df2[\"text\"] = df[\"Tokens\"]\n",
    "        df2[\"ner\"] = df[\"True\"]\n",
    "        training_file = training_file.append(df2)\n",
    "        \n",
    "    for idx, df in enumerate(test_data_as_bilou):\n",
    "        df2 = pd.DataFrame()\n",
    "        df2[\"text\"] = df[\"Tokens\"]\n",
    "        df2[\"ner\"] = df[\"True\"]\n",
    "        test_file = test_file.append(df2)\n",
    "    return training_file,test_file\n",
    "\n",
    "training,test = bilou_for_flair(custom_nlp,train,test)\n",
    "print(training.shape)\n",
    "print(test.shape)\n",
    "\n",
    "with open(\"train_res_bilou.txt\",'w+',encoding=\"utf-8\") as f:\n",
    "    training.to_csv(f,sep=\" \",encoding=\"utf-8\",index=False)\n",
    "with open(\"test_res_bilou.txt\",'w+',encoding=\"utf-8\") as f:\n",
    "    test.to_csv(f,sep=\" \",encoding=\"utf-8\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data into semantically meaningful sentences\n",
    "\n",
    "def split_bilou_into_sentences(file_name):\n",
    "    lineList = list()\n",
    "    with open(file_name + '.txt', \"r\") as f:\n",
    "        for line in f:\n",
    "            lineList.append(line)\n",
    "    finalList = list()\n",
    "\n",
    "    for index, line in enumerate(lineList):\n",
    "        if index == len(lineList) -1:\n",
    "            break\n",
    "        else:\n",
    "            if line[0] == ' ' or line[0] == '.':\n",
    "                # check if it is just a line break or really a new sentence!\n",
    "                nextLineItem = lineList[index + 1]\n",
    "                if nextLineItem.isupper() == True:\n",
    "                    finalList.append('')\n",
    "                else:\n",
    "                    pass\n",
    "            else:\n",
    "                finalList.append(line)\n",
    "\n",
    "    # writes txt-file in BILOU-Format partitioned in sentences\n",
    "    with open(file_name + '_complete.txt', 'w') as g:\n",
    "        for item in finalList:\n",
    "            if item == '':\n",
    "                g.write(\"%s\\n\" % item)\n",
    "            else:\n",
    "                g.write(\"%s\" % item)\n",
    "\n",
    "# perform splitting for training and test data\n",
    "split_bilou_into_sentences('train_res_bilou')\n",
    "split_bilou_into_sentences('test_res_bilou')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
